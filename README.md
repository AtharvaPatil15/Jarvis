# ğŸ¤– JARVIS Assistant

A production-quality voice-controlled AI assistant with an advanced holographic UI inspired by Iron Man's J.A.R.V.I.S.

![License](https://img.shields.io/badge/license-MIT-blue.svg)
![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![Node](https://img.shields.io/badge/node-18+-green.svg)

## âœ¨ Features

- ğŸ¤ **Voice Control** - Wake word detection, speech-to-text, and natural language understanding
- ğŸ¨ **3D Holographic UI** - Real-time animated sphere with state-responsive effects
- ğŸ§  **Intelligent Planning** - Multi-step task execution with LLM-powered reasoning
- ğŸ”§ **Extensible Tools** - Web search, messaging, smart search, and more
- ğŸ’¾ **Memory System** - Persistent memory and context awareness
- ğŸ”’ **Safety Layer** - Permission system for sensitive operations

## ğŸ—ï¸ Architecture

### Frontend (React + Next.js)
- **Three.js** - 3D graphics and particle systems
- **Zustand** - State management
- **Framer Motion** - Animations
- **Tailwind CSS** - Styling
- **Tone.js** - Audio analysis

### Backend (Python)
- **LM Studio** - Local LLM (Qwen 2.5 14B)
- **Porcupine** - Wake word detection
- **SpeechRecognition** - Speech-to-text
- **Edge-TTS** - Text-to-speech

## ğŸš€ Getting Started

### Prerequisites

- Python 3.8 or higher
- Node.js 18 or higher
- LM Studio (or compatible OpenAI API endpoint)
- Microphone for voice input

### Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/YOUR_USERNAME/jarvis-assistant.git
   cd jarvis-assistant
   ```

2. **Set up Python environment**
   ```bash
   python -m venv .venv
   .venv\Scripts\activate  # Windows
   # source .venv/bin/activate  # Linux/Mac
   pip install -r requirements.txt
   ```

3. **Install Node.js dependencies**
   ```bash
   npm install
   ```

4. **Configure environment**
   ```bash
   # Copy example env file
   cp .env.example .env
   
   # Add your API key to key.txt or .env file
   # Get a free Gemini API key from: https://makersuite.google.com/app/apikey
   ```

5. **Start LM Studio**
   - Download from: https://lmstudio.ai/
   - Load a model (recommended: Qwen 2.5 14B)
   - Start the server on port 1234

## ğŸ® Usage

### Frontend Development

```bash
# Install dependencies
npm install

# Run development server
npm run dev
```

Open [http://localhost:3000](http://localhost:3000) to see the UI.

### Backend (Python Voice Assistant)

```bash
# Activate virtual environment
.venv\Scripts\activate

# Run voice mode
python voice_main.py

# Run CLI mode
python main.py
```

## UI States

The holographic interface responds to different assistant states:

- **idle** - Slow breathing animation (Blue)
- **listening** - Expansion effect (Green)
- **thinking** - Chaotic rotation (Purple)
- **responding** - Normal state (Blue)
- **executing_tool** - Sharp pulsing (Red)

## Project Structure

```
jarvis-assistant/
â”œâ”€â”€ app/                    # Next.js app router
â”‚   â”œâ”€â”€ page.tsx           # Main UI page
â”‚   â”œâ”€â”€ layout.tsx         # Root layout
â”‚   â””â”€â”€ globals.css        # Global styles
â”œâ”€â”€ components/            # React components
â”‚   â”œâ”€â”€ ai-core/          # 3D scene components
â”‚   â”‚   â”œâ”€â”€ CoreSphere.tsx
â”‚   â”‚   â”œâ”€â”€ ParticleField.tsx
â”‚   â”‚   â””â”€â”€ BeamNetwork.tsx
â”‚   â””â”€â”€ overlay/          # UI overlays
â”‚       â”œâ”€â”€ AssistantText.tsx
â”‚       â””â”€â”€ ToolIndicator.tsx
â”œâ”€â”€ hooks/                # Custom React hooks
â”‚   â””â”€â”€ useAudioAnalyzer.ts
â”œâ”€â”€ store/                # Zustand stores
â”‚   â””â”€â”€ assistantStore.ts
â”œâ”€â”€ assistant/            # Python backend
â”‚   â”œâ”€â”€ orchestrator.py   # Main coordinator
â”‚   â”œâ”€â”€ brain/           # LLM logic
â”‚   â”œâ”€â”€ tools/           # Tool implementations
â”‚   â”œâ”€â”€ voice/           # Voice components
â”‚   â””â”€â”€ ui/              # PyQt6 UI (legacy)
â”œâ”€â”€ main.py              # CLI entry point
â””â”€â”€ voice_main.py        # Voice entry point
```

## Development

### Testing UI States

Use the Leva controls (top-right) to test different states:
- Change status (idle/listening/thinking/executing_tool)
- Update transcript text
- Simulate tool execution

### Connecting to Python Backend

To connect the React UI to the Python backend, you'll need to:
1. Add WebSocket support to the Python orchestrator
2. Update the Zustand store to listen for WebSocket messages

## ğŸ¤ Contributing

Contributions are welcome! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## âš ï¸ Important Notes

- **API Keys**: Never commit your API keys. Use `.env` files and add them to `.gitignore`
- **Local LLM**: For privacy, consider using LM Studio with local models
- **Wake Word**: Porcupine requires a free access key from [Picovoice Console](https://console.picovoice.ai/)

## ğŸ™ Acknowledgments

- Inspired by J.A.R.V.I.S. from the Marvel Cinematic Universe
- Built with amazing open-source technologies
- Special thanks to the AI and web development communities

## ğŸ“ Support

If you encounter issues or have questions:
- Open an [issue](https://github.com/YOUR_USERNAME/jarvis-assistant/issues)
- Check existing issues for solutions
- Read the [CONTRIBUTING.md](CONTRIBUTING.md) guide
3. Map Python states to UI states

## License

Private project - All rights reserved
